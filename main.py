import cv2
import time
import numpy as np
import os
import sys
from collections import defaultdict
import torch

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from src.config import *
from src.engine import VLMAttributeEngine
from src.utils import create_directories, save_best_samples, generate_report, show_summary_window

def main():
    # ==========================================
    # 1. ì´ˆê¸° í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
    # ==========================================
    create_directories() # ë¡œê·¸ ë° ìƒ˜í”Œ ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„±
    
    cv2.namedWindow("AI Tracking System", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("AI Tracking System", RESIZE_WIDTH, int(RESIZE_WIDTH * 0.6))
    
    import supervision as sv
    from ultralytics import YOLO

    print(f"[INFO] Loading YOLOv8 Detector from {MODEL_PATH}...")
    detector = YOLO(MODEL_PATH) # ê°ì²´ íƒì§€ ëª¨ë¸ ë¡œë“œ
    vlm_engine = VLMAttributeEngine() # VLM(CLIP) ì†ì„± ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™”
    
    # ------------------------------------------
    # [ì•ˆì „ì¥ì¹˜ ì˜ˆì™¸ì²˜ë¦¬] ì˜ìƒ ê²½ë¡œ í™•ì¸ ë¡œì§
    # ------------------------------------------
    target_video = DEFAULT_VIDEO_PATH 
    fallback_video = "data/sample.avi" # ë ˆí¬ì§€í† ë¦¬ ë‚´ ìƒì¡´ ë³´ì¥ íŒŒì¼

    if not os.path.exists(target_video):
        print(f"\nâš ï¸  [íŒŒì¼ ëˆ„ë½ ê²½ê³ ] ì„¤ì •ëœ ê²½ë¡œ '{target_video}'ì— ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        
        if os.path.exists(fallback_video):
            print(f"ğŸ”„ [ì•ˆì „ì¥ì¹˜ ê°€ë™] ê¸°ë³¸ ìƒ˜í”Œ íŒŒì¼('{fallback_video}')ë¡œ ìë™ ì „í™˜í•˜ì—¬ ì‹¤í–‰ì„ ê³„ì†í•©ë‹ˆë‹¤.\n")
            target_video = fallback_video
        else:
            print(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] ê¸°ë³¸ ìƒ˜í”Œ ì˜ìƒì¡°ì°¨ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'data/' í´ë” ë‚´ íŒŒì¼ êµ¬ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return

    cap = cv2.VideoCapture(target_video)
    if not cap.isOpened():
        print(f"[ERROR] Could not open video stream.")
        return
    # ------------------------------------------

    # [FPS Control] ì›ë³¸ ì˜ìƒ ì†ë„ì™€ ì²˜ë¦¬ ì†ë„ë¥¼ ë™ê¸°í™”í•˜ê¸° ìœ„í•œ ì„¤ì •
    raw_fps = cap.get(cv2.CAP_PROP_FPS)
    target_fps = min(raw_fps, 30.0) if raw_fps > 0 else 30.0 
    frame_interval_ms = int(1000 / target_fps)
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"[INFO] Target Video: {os.path.basename(target_video)}")
    print(f"[INFO] Speed Control: Native {raw_fps:.1f} FPS -> Sync to {target_fps:.1f} FPS")
    
    # [Tracker] ByteTrack ì´ˆê¸°í™” (ìœ ì‹¤ëœ íŠ¸ë™ì„ ë³´ê´€í•˜ëŠ” ë²„í¼ ì„¤ì • í¬í•¨)
    tracker = sv.ByteTrack(lost_track_buffer=int(TRACK_BUFFER_SEC * target_fps))

    # ë°ì´í„° ê´€ë¦¬ìš© ë³€ìˆ˜ë“¤
    track_hits = defaultdict(int) # ê°ì²´ë³„ ë“±ì¥ í”„ë ˆì„ ìˆ˜
    id_map, next_display_id = {}, 1 # Re-ID ë° ê°€ë…ì„±ì„ ìœ„í•œ ID ë§¤í•‘
    track_registry = {} # ID Fusionì„ ìœ„í•œ ìœ„ì¹˜ ì •ë³´ ì €ì¥ì†Œ
    zone_dwell, counted_ids, total_count = defaultdict(int), set(), 0 # ì¹´ìš´íŒ… ë¡œì§
    
    vlm_candidates = defaultdict(list) # ë¶„ì„ í›„ë³´ í”„ë ˆì„ ë³´ê´€ìš©
    id_attributes = {} # í™•ì •ëœ ê°ì²´ ì†ì„±
    best_crops_store = {} # ì‹ ë¢°ë„ ìƒìœ„ ìƒ˜í”Œ ë³´ê´€

    gender_stats = {"Male": 0, "Female": 0, "Unknown": 0}
    inference_log = [] # ìµœì¢… ë¦¬í¬íŠ¸ìš© ë¡œê·¸
    conf_scores_g, conf_scores_c = [], [] # í‰ê·  ì‹ ë¢°ë„ ì‚°ì¶œìš©

    frame_idx = 0
    start_time = time.time()
    fusion_dist_limit = RESIZE_WIDTH * ID_FUSION_RATIO # ID í†µí•©ì„ ìœ„í•œ ê±°ë¦¬ ì„ê³„ê°’

    print(f"[INFO] Analysis Started...")

    # ==========================================
    # 2. ë©”ì¸ ì²˜ë¦¬ ë£¨í”„ (Main Inference Loop)
    # ==========================================
    while True:
        loop_start_time = time.time()

        ret, frame = cap.read()
        if not ret: break
        frame_idx += 1

        # ì„±ëŠ¥ê³¼ ì •í™•ë„ì˜ ê· í˜•ì„ ìœ„í•œ ë¦¬ì‚¬ì´ì§•
        fh, fw = int(frame.shape[0] * (RESIZE_WIDTH/frame.shape[1])), RESIZE_WIDTH
        frame = cv2.resize(frame, (fw, fh))
        display = frame.copy()
        
        # ROI ì¹´ìš´íŒ… ë¼ì¸ ì„¤ì •
        zone_y = int(fh * ZONE_START_RATIO)
        cv2.line(display, (0, zone_y), (fw, zone_y), (0, 0, 255), 2)

        # [Detection] YOLOv8 ê°ì²´ íƒì§€ ìˆ˜í–‰
        device = 0 if torch.cuda.is_available() else "cpu"
        results = detector(frame, verbose=False, device=device)[0]        
        detections = sv.Detections.from_ultralytics(results)
        
        # [Filtering] ê°ì²´ ìœ íš¨ì„± ê²€ì¦ (ì¢…íš¡ë¹„ í•„í„°ë§ì„ í†µí•´ ë…¸ì´ì¦ˆ ì œê±°)
        valid_indices = []
        for i, xyxy in enumerate(detections.xyxy):
            x1, y1, x2, y2 = xyxy
            bw, bh = x2 - x1, y2 - y1
            aspect_ratio = bh / bw if bw > 0 else 0
            
            # ì‚¬ëŒì´ ê±·ëŠ” ìì„¸ë¥¼ ê³ ë ¤í•œ ì¢…íš¡ë¹„(1.15) ë° ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
            if aspect_ratio > 1.15 and detections.confidence[i] >= CONF_THRESH:
                valid_indices.append(i)
        
        detections = detections[valid_indices]
        # [Tracking] ByteTrackì„ í†µí•œ ì‹¤ì‹œê°„ ì¶”ì  ê°±ì‹ 
        detections = tracker.update_with_detections(detections)

        active_now_ids = set()

        if detections.tracker_id is not None:
            for xyxy, raw_id in zip(detections.xyxy, detections.tracker_id):
                raw_id = int(raw_id)
                track_hits[raw_id] += 1
                
                # ì•ˆì •ì ì¸ íŠ¸ë˜í‚¹ì´ í™•ë³´ëœ í”„ë ˆì„(3íšŒ ì´ìƒ)ë¶€í„° ì²˜ë¦¬ ì‹œì‘
                if track_hits[raw_id] < 3: continue 

                x1, y1, x2, y2 = map(int, xyxy)
                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2

                # ==========================================
                # 3. ID Fusion & Re-ID Logic (ID ì¤‘ë³µ ìƒì„± ë°©ì§€)
                # ==========================================
                if raw_id not in id_map:
                    found_legacy = False
                    for disp_id, data in list(track_registry.items()):
                        if disp_id in active_now_ids: continue # í˜„ì¬ í™œì„±í™”ëœ ê°ì²´ëŠ” ìŠ¤í‚µ
                        
                        # ì¼ì • ì‹œê°„(FORGET_TIME) ì´ë‚´ì— ì‚¬ë¼ì§„ ê°ì²´ë§Œ ì¬ë§¤ì¹­ ì‹œë„
                        if frame_idx - data["last_frame"] > (FORGET_TIME_SEC * target_fps):
                            if disp_id in track_registry: del track_registry[disp_id]
                            continue
                        
                        # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ ê·¼ì ‘ë„ ì¸¡ì • (Re-ID)
                        dist = np.sqrt((cx - data["pos"][0])**2 + (cy - data["pos"][1])**2)
                        if dist < fusion_dist_limit:
                            id_map[raw_id] = disp_id
                            found_legacy = True
                            break
                    if not found_legacy:
                        id_map[raw_id] = next_display_id
                        next_display_id += 1
                
                tid = id_map[raw_id]
                active_now_ids.add(tid)
                track_registry[tid] = {"pos": (cx, cy), "last_frame": frame_idx}

                # ==========================================
                # 4. VLM Attribute Analysis (ë‹¤ë‹¨ê³„ ì‹œê³„ì—´ íˆ¬í‘œ)
                # ==========================================
                if tid not in id_attributes:
                    # ì—°ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ 3, 10, 20 í”„ë ˆì„ ì‹œì ì—ì„œë§Œ ë¶„ì„ ìˆ˜í–‰
                    if track_hits[raw_id] in [3, 10, 20]:
                        crop = frame[max(0,y1):min(fh,y2), max(0,x1):min(fw,x2)]
                        if crop.size > 0:
                            res = vlm_engine.analyze(crop) # VLM/HSV í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ 
                            if res: vlm_candidates[tid].append({'res': res, 'img': crop.copy()})

                    # 31í”„ë ˆì„ ì‹œì ì—ì„œ ìˆ˜ì§‘ëœ í›„ë³´êµ° ì¤‘ ê°€ì¥ ì‹ ë¢°ë„ê°€ ë†’ì€ ê²°ê³¼ë¥¼ ìµœì¢… í™•ì •
                    if track_hits[raw_id] == 31:
                        candidates = vlm_candidates[tid]
                        if candidates:
                            # ì‹ ë¢°ë„(Confidence) ê¸°ë°˜ ìµœì  í”„ë ˆì„ ì„ íƒ
                            best_entry = max(candidates, key=lambda x: x['res']['g_cf'])
                            best_res = best_entry['res']
                            
                            final_g, final_c = best_res["g"], best_res["c"]
                            g_conf, c_conf = best_res["g_cf"], best_res["c_cf"]
                            vlm_desc = best_res.get("desc", "")

                            if g_conf < 50.0: final_g = "Unknown" # ì €ì‹ ë¢°ë„ ë°ì´í„° í•„í„°ë§
                            
                            id_attributes[tid] = (final_g, final_c, g_conf, c_conf)
                            gender_stats[final_g] += 1
                            
                            conf_scores_g.append(g_conf)
                            conf_scores_c.append(c_conf)
                            
                            log_str = f"[ID {tid:02d}] Result: {final_g}/{final_c} (G:{g_conf:.1f}% C:{c_conf:.1f}%) | {vlm_desc}"
                            inference_log.append(log_str)
                            
                            # ìš°ìˆ˜ì„± ì…ì¦ìš© ìƒìœ„ ìƒ˜í”Œ ì €ì¥ì†Œì— ì¶”ê°€
                            best_crops_store[tid] = {
                                'conf': g_conf,
                                'img': best_entry['img'],
                                'label': f"{final_g}_{final_c}"
                            }

                # ==========================================
                # 5. ROI ì¹´ìš´íŒ… ë° í™”ë©´ ì¶œë ¥ (Dwell-Time ê¸°ë°˜)
                # ==========================================
                attr = id_attributes.get(tid, ("Analyzing", "...", 0, 0))
                label = f"ID:{tid} | {attr[0][:1]}({attr[2]:.0f}%) | {attr[1]}({attr[3]:.0f}%)"

                # íŒì •ì„ (ROI) ì•„ë˜ì— ì¡´ì¬í•˜ë©° ì¼ì • ì‹œê°„(Dwell Time) ìœ ì§€ ì‹œ ì¹´ìš´íŠ¸
                if tid not in counted_ids and cy > zone_y:
                    zone_dwell[tid] += 1
                    if zone_dwell[tid] >= (DWELL_TIME_SEC * target_fps):
                        counted_ids.add(tid)
                        total_count += 1

                # ì‹œê°í™” (ì¹´ìš´íŠ¸ëœ ê°ì²´ëŠ” ë…¹ìƒ‰ ë°•ìŠ¤ í‘œì‹œ)
                color_box = (0, 255, 0) if tid in counted_ids else (255, 255, 255)
                cv2.rectangle(display, (x1, y1), (x2, y2), color_box, 2)
                cv2.putText(display, label, (x1, y1 - 10), 1, 0.8, color_box, 1)

        # [HUD Overlay] ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´ ì¶œë ¥
        overlay = display.copy()
        cv2.rectangle(overlay, (0, 0), (450, 160), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, display, 0.4, 0, display)
        cv2.putText(display, f"VLM ROBUST SYSTEM: {os.path.basename(target_video)}", (20, 45), 1, 1.5, (255, 255, 255), 2)
        cv2.putText(display, f"MALE : {gender_stats['Male']} | FEMALE : {gender_stats['Female']}", (20, 95), 1, 1.2, (0, 255, 255), 1)
        
        elapsed_total = time.time() - start_time
        real_fps = frame_idx / elapsed_total if elapsed_total > 0 else 0
        prog = int(frame_idx/total_frames*100) if total_frames > 0 else 0
        cv2.putText(display, f"PROG : {prog}% | FPS : {real_fps:.1f} (Limit: {target_fps:.0f})", (20, 135), 1, 0.9, (150, 150, 150), 1)

        cv2.imshow("AI Tracking System", display)
        
        # FPS ì œì–´ ë° ë£¨í”„ íƒˆì¶œ ì¡°ê±´
        proc_time_ms = (time.time() - loop_start_time) * 1000
        wait_ms = max(1, int(frame_interval_ms - proc_time_ms))
        if cv2.waitKey(wait_ms) & 0xFF == ord("q"): break

    cap.release()
    cv2.destroyAllWindows()
    
    # ==========================================
    # 6.End-of-Stream Flush ë¡œì§
    # ==========================================
    # ì˜ìƒ ì¢…ë£Œ ì‹œ ë¶„ì„ ì¤‘ì´ë˜ ì”ì—¬ ë²„í¼ ë°ì´í„°ë¥¼ ê°•ì œë¡œ í™•ì • ë° ì¹´ìš´íŠ¸ ë°˜ì˜
    print(f"\n[INFO] End of Stream detected. Flushing remaining buffers...")
    
    for tid, candidates in vlm_candidates.items():
        if tid in id_attributes or not candidates: continue

        best_entry = max(candidates, key=lambda x: x['res']['g_cf'])
        best_res = best_entry['res']
        final_g, final_c = best_res["g"], best_res["c"]
        g_conf, c_conf = best_res["g_cf"], best_res["c_cf"]

        if g_conf < 50.0: final_g = "Unknown"
        
        id_attributes[tid] = (final_g, final_c, g_conf, c_conf)
        gender_stats[final_g] += 1
        conf_scores_g.append(g_conf)
        conf_scores_c.append(c_conf)
        
        inference_log.append(f"[ID {tid:02d}] Result: {final_g}/{final_c} (G:{g_conf:.1f}% C:{c_conf:.1f}%) [FLUSHED]")
        best_crops_store[tid] = {'conf': g_conf, 'img': best_entry['img'], 'label': f"{final_g}_{final_c}"}

        # ì˜ìƒ ëë¶€ë¶„ì—ì„œ ë“±ì¥í•œ ê°ì²´ë„ ì¹´ìš´íŒ… ëˆ„ë½ ì—†ì´ ë°˜ì˜
        if tid not in counted_ids:
            counted_ids.add(tid)
            total_count += 1

    # ìµœì¢… ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± ë° ë¦¬í¬íŠ¸ ìœˆë„ìš° í‘œì‹œ
    elapsed = time.time() - start_time
    final_fps = frame_idx/elapsed
    avg_g_conf = np.mean(conf_scores_g) if conf_scores_g else 0.0
    avg_c_conf = np.mean(conf_scores_c) if conf_scores_c else 0.0
    
    video_base_name = os.path.splitext(os.path.basename(target_video))[0]
    save_best_samples(video_base_name, best_crops_store)
    generate_report(video_base_name, total_count, gender_stats, avg_g_conf, avg_c_conf, final_fps, inference_log)
    show_summary_window(video_base_name, total_count, gender_stats, avg_g_conf, avg_c_conf, final_fps)

if __name__ == "__main__":
    main()